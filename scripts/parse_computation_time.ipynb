{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.path.exists(base_root): True\n",
      "path: ../231115_computation_time/231115_computation_timetableshift_heloc/sar/MLP/shift_type_None_shift_severity_1/231115_computation_time_seed_0_dataset_heloc_sf0.1_upth0.75_lowth0.25.txt\n",
      "source_model_train_time_match: 25.384751319885254\n",
      "gnn_train_time_match: 25.384751319885254\n",
      "total_inference_time_match: 0.03383636474609375\n",
      "total_adaptation_time: 0.1282179355621338\n",
      "avg_inference_time: 0.0001146995415121822\n",
      "avg_adaptation_time: 0.00043463706970214844\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../231115_computation_time/231115_computation_timeopenml-cc18_cmc/eata/FTTransformer/shift_type_Gaussian_shift_severity_0.1/231115_computation_time_seed_0_dataset_cmc_sf0.0_upth0.9_lowth0.25.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_716534/474438771.py\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_str\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shift_type_Gaussian_shift_severity_0.1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"231115_computation_time_seed_0_dataset_cmc_sf0.0_upth0.9_lowth0.25.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0msource_model_train_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"source_model_training_time: (\\d+\\.\\d+)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../231115_computation_time/231115_computation_timeopenml-cc18_cmc/eata/FTTransformer/shift_type_Gaussian_shift_severity_0.1/231115_computation_time_seed_0_dataset_cmc_sf0.0_upth0.9_lowth0.25.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# for path in Path('src').rglob('*.c'):\n",
    "#     print(path.name)\n",
    "\n",
    "\n",
    "# for model in ['mlp', 'tabnet', 'fttransformer']:\n",
    "#     for dataset in ['cmc', 'mfeat-pixel', 'dna', 'heloc', 'diabetes_readmission', 'anes']:\n",
    "#         model_str = {\"mlp\": \"MLP\", \"tabnet\": \"TabNet\", \"fttransformer\": \"FTTransformer\"}\n",
    "\n",
    "#         path = Path(\"../231115_computation_time\")\n",
    "#         source_model_train_time = re.compile(f\"source_model_training_time: (\\d+\\.\\d+)\")\n",
    "#         gnn_train_time = re.compile(f\"gnn_training_time: (\\d+\\.\\d+)\")\n",
    "#         total_inference_time = re.compile(f\"total_inference_time: (\\d+\\.\\d+)\")\n",
    "#         total_adaptation_time = re.compile(f\"total_adaptation_time: (\\d+\\.\\d+)\")\n",
    "#         avg_inference_time = re.compile(f\"avg_inference_time: (\\d+\\.\\d+)\")\n",
    "#         avg_adaptation_time = re.compile(f\"avg_adaptation_time: (\\d+\\.\\d+)\")\n",
    "\n",
    "#         for p in path.rglob(\"*.txt\"):\n",
    "#             f = open(p, \"r\").read()\n",
    "#             source_model_train_time_match = re.findall(source_model_train_time, f)\n",
    "#             gnn_train_time_match = re.findall(source_model_train_time, f)\n",
    "#             total_inference_time_match = re.findall(total_inference_time, f)\n",
    "#             total_adaptation_time_match = re.findall(total_adaptation_time, f)\n",
    "#             avg_inference_time_match = re.findall(avg_inference_time, f)\n",
    "#             avg_adaptation_time_match = re.findall(avg_adaptation_time, f)\n",
    "\n",
    "#             if source_model_train_time_match:\n",
    "#                 print(f\"path: {p}\")\n",
    "#                 print(f\"source_model_train_time_match: {source_model_train_time_match[-1]}\")\n",
    "#                 print(f\"gnn_train_time_match: {gnn_train_time_match[-1]}\")\n",
    "#                 print(f\"total_inference_time_match: {total_inference_time_match[-1]}\")\n",
    "#                 print(f\"total_adaptation_time: {total_adaptation_time_match[-1]}\")\n",
    "#                 print(f\"avg_inference_time: {avg_inference_time_match[-1]}\")\n",
    "#                 print(f\"avg_adaptation_time: {avg_adaptation_time_match[-1]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base_root = \"../231115_computation_time/231115_computation_timeopenml-cc18_cmc/calibrator_label_distribution_handler/FTTransformer/shift_type_Gaussian_shift_severity_0.1/231115_computation_time_seed_0_dataset_cmc_sf0.0_upth0.9_lowth0.25.txt\"\n",
    "base_root = \"../231115_computation_time/231115_computation_timeopenml-cc18_cmc\"\n",
    "\n",
    "print(f\"os.path.exists(base_root): {os.path.exists(base_root)}\")\n",
    "\n",
    "for model in ['fttransformer']:\n",
    "    for dataset in ['cmc']:\n",
    "        for method in ['calibrator_label_distribution_handler', 'eata', 'em', 'lame', 'pl', 'sam', 'sar', 'ttt++']:\n",
    "            model_str = {\"mlp\": \"MLP\", \"tabnet\": \"TabNet\", \"fttransformer\": \"FTTransformer\"}\n",
    "\n",
    "            path = os.path.join(base_root, method, model_str[model], \"shift_type_Gaussian_shift_severity_0.1\", \"231115_computation_time_seed_0_dataset_cmc_sf0.0_upth0.9_lowth0.25.txt\")\n",
    "            f = open(path, \"r\").read()\n",
    "\n",
    "            source_model_train_time = re.compile(f\"source_model_training_time: (\\d+\\.\\d+)\")\n",
    "            gnn_train_time = re.compile(f\"gnn_training_time: (\\d+\\.\\d+)\")\n",
    "            total_inference_time = re.compile(f\"total_inference_time: (\\d+\\.\\d+)\")\n",
    "            total_adaptation_time = re.compile(f\"total_adaptation_time: (\\d+\\.\\d+)\")\n",
    "            avg_inference_time = re.compile(f\"avg_inference_time: (\\d+\\.\\d+)\")\n",
    "            avg_adaptation_time = re.compile(f\"avg_adaptation_time: (\\d+\\.\\d+)\")\n",
    "\n",
    "            source_model_train_time_match = re.findall(source_model_train_time, f)\n",
    "            gnn_train_time_match = re.findall(source_model_train_time, f)\n",
    "            total_inference_time_match = re.findall(total_inference_time, f)\n",
    "            total_adaptation_time_match = re.findall(total_adaptation_time, f)\n",
    "            avg_inference_time_match = re.findall(avg_inference_time, f)\n",
    "            avg_adaptation_time_match = re.findall(avg_adaptation_time, f)\n",
    "\n",
    "            if source_model_train_time_match:\n",
    "                print(f\"path: {p}\")\n",
    "                print(f\"source_model_train_time_match: {source_model_train_time_match[-1]}\")\n",
    "                print(f\"gnn_train_time_match: {gnn_train_time_match[-1]}\")\n",
    "                print(f\"total_inference_time_match: {total_inference_time_match[-1]}\")\n",
    "                print(f\"total_adaptation_time: {total_adaptation_time_match[-1]}\")\n",
    "                print(f\"avg_inference_time: {avg_inference_time_match[-1]}\")\n",
    "                print(f\"avg_adaptation_time: {avg_adaptation_time_match[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: ../log/231116_new_hyperparameter_sensitivity/mlp_heloc_0_ours_smoothing_factor_0tableshift_heloc/calibrator_label_distribution_handler/MLP/shift_type_None_shift_severity_1/mlp_heloc_0_ours_smoothing_factor_0_seed_0_dataset_heloc_sf0.1_upth0.75_lowth0.25.txt\n",
      "os.path.exists(filename): False\n",
      "['mlp_heloc_0_ours_smoothing_factor_0_seed_0_dataset_heloc_sf0_upth0.75_lowth0.25.txt']\n",
      "-1\n",
      "filename: ../log/231116_new_hyperparameter_sensitivity/mlp_heloc_0_ours_smoothing_factor_0.1tableshift_heloc/calibrator_label_distribution_handler/MLP/shift_type_None_shift_severity_1/mlp_heloc_0_ours_smoothing_factor_0.1_seed_0_dataset_heloc_sf0.1_upth0.75_lowth0.25.txt\n",
      "os.path.exists(filename): True\n",
      "['mlp_heloc_0_ours_smoothing_factor_0.1_seed_0_dataset_heloc_sf0.1_upth0.75_lowth0.25.txt']\n",
      "{'before_adaptation_acc': -1, 'after_adaptation_acc': -1}\n",
      "filename: ../log/231116_new_hyperparameter_sensitivity/mlp_heloc_0_ours_smoothing_factor_0.2tableshift_heloc/calibrator_label_distribution_handler/MLP/shift_type_None_shift_severity_1/mlp_heloc_0_ours_smoothing_factor_0.2_seed_0_dataset_heloc_sf0.1_upth0.75_lowth0.25.txt\n",
      "os.path.exists(filename): False\n",
      "['mlp_heloc_0_ours_smoothing_factor_0.2_seed_0_dataset_heloc_sf0.2_upth0.75_lowth0.25.txt']\n",
      "-1\n",
      "filename: ../log/231116_new_hyperparameter_sensitivity/mlp_heloc_0_ours_smoothing_factor_0.3tableshift_heloc/calibrator_label_distribution_handler/MLP/shift_type_None_shift_severity_1/mlp_heloc_0_ours_smoothing_factor_0.3_seed_0_dataset_heloc_sf0.1_upth0.75_lowth0.25.txt\n",
      "os.path.exists(filename): False\n",
      "['mlp_heloc_0_ours_smoothing_factor_0.3_seed_0_dataset_heloc_sf0.3_upth0.75_lowth0.25.txt']\n",
      "-1\n",
      "filename: ../log/231116_new_hyperparameter_sensitivity/mlp_heloc_0_ours_smoothing_factor_0.4tableshift_heloc/calibrator_label_distribution_handler/MLP/shift_type_None_shift_severity_1/mlp_heloc_0_ours_smoothing_factor_0.4_seed_0_dataset_heloc_sf0.1_upth0.75_lowth0.25.txt\n",
      "os.path.exists(filename): False\n",
      "['mlp_heloc_0_ours_smoothing_factor_0.4_seed_0_dataset_heloc_sf0.4_upth0.75_lowth0.25.txt']\n",
      "-1\n",
      "filename: ../log/231116_new_hyperparameter_sensitivity/mlp_heloc_0_ours_smoothing_factor_0.5tableshift_heloc/calibrator_label_distribution_handler/MLP/shift_type_None_shift_severity_1/mlp_heloc_0_ours_smoothing_factor_0.5_seed_0_dataset_heloc_sf0.1_upth0.75_lowth0.25.txt\n",
      "os.path.exists(filename): False\n",
      "['mlp_heloc_0_ours_smoothing_factor_0.5_seed_0_dataset_heloc_sf0.5_upth0.75_lowth0.25.txt']\n",
      "-1\n",
      "filename: ../log/231116_new_hyperparameter_sensitivity/mlp_heloc_0_ours_smoothing_factor_0.6tableshift_heloc/calibrator_label_distribution_handler/MLP/shift_type_None_shift_severity_1/mlp_heloc_0_ours_smoothing_factor_0.6_seed_0_dataset_heloc_sf0.1_upth0.75_lowth0.25.txt\n",
      "os.path.exists(filename): False\n",
      "['mlp_heloc_0_ours_smoothing_factor_0.6_seed_0_dataset_heloc_sf0.6_upth0.75_lowth0.25.txt']\n",
      "-1\n",
      "filename: ../log/231116_new_hyperparameter_sensitivity/mlp_heloc_0_ours_smoothing_factor_0.7tableshift_heloc/calibrator_label_distribution_handler/MLP/shift_type_None_shift_severity_1/mlp_heloc_0_ours_smoothing_factor_0.7_seed_0_dataset_heloc_sf0.1_upth0.75_lowth0.25.txt\n",
      "os.path.exists(filename): False\n",
      "['mlp_heloc_0_ours_smoothing_factor_0.7_seed_0_dataset_heloc_sf0.7_upth0.75_lowth0.25.txt']\n",
      "-1\n",
      "filename: ../log/231116_new_hyperparameter_sensitivity/mlp_heloc_0_ours_smoothing_factor_0.8tableshift_heloc/calibrator_label_distribution_handler/MLP/shift_type_None_shift_severity_1/mlp_heloc_0_ours_smoothing_factor_0.8_seed_0_dataset_heloc_sf0.1_upth0.75_lowth0.25.txt\n",
      "os.path.exists(filename): False\n",
      "['mlp_heloc_0_ours_smoothing_factor_0.8_seed_0_dataset_heloc_sf0.8_upth0.75_lowth0.25.txt']\n",
      "-1\n",
      "filename: ../log/231116_new_hyperparameter_sensitivity/mlp_heloc_0_ours_smoothing_factor_0.9tableshift_heloc/calibrator_label_distribution_handler/MLP/shift_type_None_shift_severity_1/mlp_heloc_0_ours_smoothing_factor_0.9_seed_0_dataset_heloc_sf0.1_upth0.75_lowth0.25.txt\n",
      "os.path.exists(filename): False\n",
      "['mlp_heloc_0_ours_smoothing_factor_0.9_seed_0_dataset_heloc_sf0.9_upth0.75_lowth0.25.txt']\n",
      "-1\n",
      "filename: ../log/231116_new_hyperparameter_sensitivity/mlp_heloc_0_ours_smoothing_factor_1tableshift_heloc/calibrator_label_distribution_handler/MLP/shift_type_None_shift_severity_1/mlp_heloc_0_ours_smoothing_factor_1_seed_0_dataset_heloc_sf0.1_upth0.75_lowth0.25.txt\n",
      "os.path.exists(filename): False\n",
      "['mlp_heloc_0_ours_smoothing_factor_1_seed_0_dataset_heloc_sf1_upth0.75_lowth0.25.txt']\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def get_avg_online_acc(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        # Define the regex patterns\n",
    "        before_adaptation_pattern = r'before adaptation \\| acc: (\\d+\\.\\d+)'\n",
    "        after_adaptation_pattern = r'after adaptation \\| acc: (\\d+\\.\\d+)'\n",
    "\n",
    "        # Perform the regex search on the entire file content\n",
    "        before_adaptation_match = re.findall(before_adaptation_pattern, content)\n",
    "        after_adaptation_match = re.findall(after_adaptation_pattern, content)\n",
    "\n",
    "        if before_adaptation_match and after_adaptation_match:\n",
    "            before_adaptation_test_acc = before_adaptation_match[-1]\n",
    "            after_adaptation_test_acc = after_adaptation_match[-1]\n",
    "            return {\n",
    "                'before_adaptation_acc': before_adaptation_test_acc,\n",
    "                'after_adaptation_acc': after_adaptation_test_acc,\n",
    "            }\n",
    "        else:\n",
    "            # raise NotImplementedError\n",
    "            return {\n",
    "                'before_adaptation_acc': -1,\n",
    "                'after_adaptation_acc': -1,\n",
    "            }\n",
    "    else:\n",
    "        # raise NotImplementedError\n",
    "        return -1\n",
    "\n",
    "\n",
    "\n",
    "log_prefix = \"../log/231116_new_hyperparameter_sensitivity\"\n",
    "smoothing_factor_list = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "for smoothing_factor in smoothing_factor_list:\n",
    "    filename = os.path.join(\n",
    "        log_prefix,\n",
    "        f\"mlp_heloc_0_ours_smoothing_factor_{smoothing_factor}tableshift_heloc\",\n",
    "        f\"calibrator_label_distribution_handler/MLP/shift_type_None_shift_severity_1/mlp_heloc_0_ours_smoothing_factor_{smoothing_factor}_seed_0_dataset_heloc_sf0.1_upth0.75_lowth0.25.txt\"\n",
    "    )\n",
    "    print(f\"filename: {filename}\")\n",
    "    print(f\"os.path.exists(filename): {os.path.exists(filename)}\")\n",
    "    # print(os.listdir(os.path.join(log_prefix, f\"mlp_heloc_0_ours_smoothing_factor_{smoothing_factor}tableshift_heloc\", f\"calibrator_label_distribution_handler/MLP/shift_type_None_shift_severity_1/mlp_heloc_0_ours_smoothing_factor_{smoothing_factor}_seed_0_dataset_heloc_sf0.2_upth0.75_lowth0.25.txt\")))\n",
    "    print(os.listdir(os.path.join(log_prefix, f\"mlp_heloc_0_ours_smoothing_factor_{smoothing_factor}tableshift_heloc\", f\"calibrator_label_distribution_handler/MLP/shift_type_None_shift_severity_1\")))\n",
    "    print(get_avg_online_acc(filename))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
