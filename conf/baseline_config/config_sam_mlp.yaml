### model
## currently supported: MLP, TabNet, TabTransformer, FTTransformer
## supervised baseline: lr, knn, rf, xgboost, catboost
model: MLP
mlp:
  embedding: false
  num_enc_layers: 2
  hidden_dim: [256, 256] # either int or list of int
  dropout_rate: 0
  bn: false

### dataset and preprocessing
## currently available dataset list
# openml-cc18: adult, cmc, mfeat-karhunen, optdigits, diabetes, semeion, mfeat-pixel, dna
# tableshift: heloc(O), diabetes_readmission(O), anes(O), mooc(O), acsincome(O, 매우 느림), acsfoodstamps(X), acspubcov(X), acsunemployment(O, 매우 느림), brfss_diabetes, brfss_blood_pressure(X), communities_and_crime(X), compas(X/test를 써야함), german(X/test를 써야함), nhanes_cholesterol(X), nhanes_lead(X), physionet, mimic_extract_los_3, mimic_extract_mort_hosp
# openml-regression: abalone, cholestrol
# shifts: weather_reg, weather_cls, power
# folktables: state, time, time_state
benchmark: openml-cc18 # tableshift, openml-cc18, folktables, shifts, openml-regression, scikit-learn
dataset: cmc
shift_type: numerical # null, Gaussian, uniform, random_drop, column_drop, numerical, categorical
shift_severity: 0.2
normalizer: StandardScaler
missing_imputation_method: emd # currently supported: zero, mean, emd

### hyperparameters for pretraining
pretrain_epochs: 50
pretrain_batch_size: 64
pretrain_optimizer: AdamW
pretrain_lr: 1e-4
pretrain_mask_ratio: 0.75

### hyperparameters for training
retrain: true
epochs: 50
train_batch_size: 64
train_optimizer: AdamW
train_lr: 1e-4
train_ratio: 1 # for evaluating on limited number of train instances
smote: false
train_patience: 10

# hyperparmeters for posttraining
posttrain_lr: 5e-3
posttrain_epochs: 50
posttrain_patience: 10
posttrain_shrinkage_factor: 0.5

### hyperparameters for tta
episodic: true # reset model to the pretrained one for every batch if true
num_steps: 1 # optimization steps for each batch
test_lr: 1e-3
test_batch_size: 64
test_optimizer: AdamW
train_params: [all] # currently supported: all, LN, BN, GN

##################################################################
### methods & other hyperparameters
method: [sam]
# search space - episodic, test_lr : 1e-3~1e-5, num_steps : 1, 5, 10
# em(tent), sam, sar, pl, ttt++, eata, memo
# em - true, 1e-4, 1
# sam - true, 1e-3, 1
# sar - true, 1e-3, 1
# pl - true, 1e-4, 1
# ttt++ - episodic=true test_lr=1e-5 num_steps=10
# eata - episodic=true test_lr=1e-5 num_steps=10
# memo - episodic=true test_lr=1e-5 num_steps=10


# for masked autoencoders
mae_imputation_method: emd # imputation for unnormalized data: zero, mean, emd
test_mask_ratio: 0.75
delta: 1e-4

# for other baselines
temp: 1 # temperature scaling for entropy minimization methods
renyi_entropy_alpha: 1.5 # for generalized entropy minimization
ns_threshold: 0.04 # negative sampling
ns_weight: 0.5
kld_weight: 0.1 # for kl-divergence loss
memo_aug_num: 64 # for memo
dropout_steps: 0 # for differential entropy minimization
eata_e_margin: 2.4538776394910684 # for eata
eata_d_margin: 0.05 # for eata
ttt_coef: [1.0, 0.1, 1.0] # for ttt++

smoothing_factor: 0.1 # for moving average (high smoothing factor uses previous one much) # TODO: tune for tta
uncertainty_upper_percentile_threshod: 0.75 # TODO: tune for tta
uncertainty_lower_percentile_threshod: 0.25 # TODO: tune for tta

# for supervised baselines
num_estimators: 25
max_depth: 3

##################################################################
### device
device: cuda
gpu_idx: 0

### seed for reproduciblity
seed: 0

### log_dir for logging, out_dir for saving model, and vis_dir for visualization
log_dir: log
log_prefix: log_final
out_dir: exps/test_mae
vis_dir: imgs

# visualization
vis: false
entropy_gradient_vis: false

# graph
graph_add_noise: false
graph_noise_std: 0.1

### wandb
wandb_user: drumpt

### slack_notification
slack_token: xoxb-5074113602964-5068716983253-VPOnZMWgclHFEjqLVgW4KlYd
slack_channel: experiment

##################################################################
### not to create outputs directory for hydra
defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

hydra:
  output_subdir: null
  run:
    dir: .