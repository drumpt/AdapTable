### model
## currently supported: MLP, TabNet, TabTransformer
## supervised baseline: lr, knn, rf, xgboost, catboost
model: MLP
mlp:
  use_embedding: false
  num_layers: 4
  hidden_dim: [256, 256, 256] # either int or list of int
  dropout_rate: 0
  use_bn: false

### dataset
benchmark: tableshift # tableshift, openml-cc18, openml-regression, scikit-learn, shifts, folktables
## currently available dataset list
# openml-cc18: adult(income), cmc, mfeat-karhunen(karhunen), optdigits, diabetes, semeion, mfeat-pixel, dna
# tableshift: diabetes_readmission(O), heloc(O), acsincome, acsfoodstamps(X), acspubcov(X), acsunemployment(X), brfss_diabetes, brfss_blood_pressure(X), communities_and_crime(X), compas(X/test를 써야함), german(X/test를 써야함), nhanes_cholesterol(X), nhanes_lead(X), physionet, anes(X), mimic_extract_los_3, mimic_extract_mort_hosp, mooc
# openml-regression: abalone, cholestrol
# shifts: weather_reg, weather_cls, power
# folktables: state, time, time_state
dataset: heloc
shift_type: null # null, Gaussian, uniform, random_drop, column_drop, numerical, categorical
shift_severity: 0
normalizer: StandardScaler
imputation_method: emd # zero, mean, emd(from empirical marginal distribution)
train_ratio: 1 # for evaluating on limited number of train instances

### hyperparameters for MAE pretraining
pretrain_epochs: 50
pretrain_batch_size: 64
pretrain_optimizer: AdamW
pretrain_lr: 1e-4
pretrain_mask_ratio: 0.75

### hyperparameters for training
retrain: true
epochs: 50
train_batch_size: 64
train_optimizer: AdamW
train_lr: 1e-4

### hyperparameters for test-time adaptation
episodic: true # reset model to the pretrained one for every batch if true
num_steps: 20 # optimization steps for each batch
test_batch_size: 64
test_optimizer: AdamW
train_params: [all] # currently supported: all, LN, BN, GN
test_lr: 1e-4

##################################################################
### methods & other hyperparameters
## currently no baselines: em, sar, memo, sgem(gem + ns)
method: [em] # currently supported: mae, em, sar, memo, dem, gem, ns, dm, kld

# for masked autoencoders
test_mask_ratio: 0.75
delta: 1e-4

# for other baselines
temp: 1 # temperature scaling for entropy minimization methods
renyi_entropy_alpha: 1.5 # for generalized entropy minimization
ns_threshold: 0.04 # negative sampling
ns_weight: 0.5
kld_weight: 0.1 # for kl-divergence loss
memo_aug_num: 64 # for memo
dropout_steps: 0 # for differential entropy minimization
eata_e_margin: 2.4538776394910684 # for eata
eata_d_margin: 0.05 # for eata
ttt_coef: [1.0, 0.1, 1.0] # for ttt++

# for supervised baselines
num_estimators: 25
max_depth: 3

##################################################################
### device
device: cuda
gpu_idx: 0

### seed for reproduciblity
seed: 0

### output dir for saving model and log_dir for logging
out_dir: exps/test_mae
log_dir: log
log_prefix: log_final

### tsne
tsne: false
tsne_dir: tsne

### wandb
wandb_user: drumpt

### slack_notification
slack_token: xoxb-5074113602964-5068716983253-VPOnZMWgclHFEjqLVgW4KlYd
slack_channel: experiment

##################################################################
### not to create outputs directory for hydra
defaults:  
  - _self_  
  - override hydra/hydra_logging: disabled  
  - override hydra/job_logging: disabled  
  
hydra:  
  output_subdir: null  
  run:  
    dir: .